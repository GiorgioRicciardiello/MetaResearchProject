# Meta Research Scripts

This repository contains three Python scripts designed for literature research. Each script targets different data sources and employs various techniques for searching, filtering, and extracting information.

## Table of Contents

- [Overview](#overview)
- [Requirements](#requirements)
- [Installation](#installation)
- [Scripts Overview](#scripts-overview)
  - [MetaResearchStep1_FindPapers.py](#metaresearchstep1_findpaperspy)
  - [MetaResearchStep2_GetFullAbstracts.py](#metaresearchstep2_getfullabstractspy)
  - [ScrapperPubMed.py](#scrapperpubmedpy)
- [Usage](#usage)
- [Notes](#notes)

## Overview

The repository contains three primary scripts:


1. **config.py**
   File configurations, API keys, QUERY are in this python script.

2. **MetaResearchStep1_FindPapers.py**  
   Searches Google Scholar via SerpAPI using a predefined query, filters results based on inclusion/exclusion criteria, removes duplicate entries using fuzzy string matching, and saves relevant paper details to a CSV file.

3. **MetaResearchStep2_GetFullAbstracts.py**  
   Reads the CSV file generated by the first script and uses Selenium to navigate to paper links. It leverages a Hugging Face model (e.g., Llama-2-7B-chat-hf) to extract a concise abstract from the page text, then saves the updated results to a new CSV file.

4. **ScrapperPubMed.py**  
   Scrapes PubMed for papers matching a specified query. It retrieves paper titles, links, and abstracts by parsing the HTML of each paperâ€™s page, then saves the results to a CSV file. The script prompts the user to input the number of pages to scrape.

## Requirements

- **Python 3.6+**
- **Libraries/Packages:**
  - `requests`
  - `pandas`
  - `spacy` (with the `"en_core_web_sm"` model)
  - `fuzzywuzzy`
  - `selenium`
  - `beautifulsoup4`
  - `transformers`
  - `torch`
  - `csv` (Python standard library)
- **Other Dependencies:**
  - A valid SerpAPI key for Google Scholar queries.
  - Chrome browser and the appropriate [ChromeDriver](https://sites.google.com/a/chromium.org/chromedriver/) for Selenium.
  - A Hugging Face token for accessing the chosen model.

## Installation

1. **Clone the Repository.**
2. **Install Python Dependencies.**
3. **Set Up External Tools:**

   - **SerpAPI:** Replace the `SERPAPI_KEY` in `MetaResearchStep1_FindPapers.py` with your actual API key.
   - **Selenium:** Ensure that the ChromeDriver path in `MetaResearchStep2_GetFullAbstracts.py` is correctly set according to your system.
   - **Hugging Face:** Replace the token (`hf_token`) in `MetaResearchStep2_GetFullAbstracts.py` with your personal Hugging Face token.

## Scripts Overview

### MetaResearchStep1_FindPapers.py

- **Functionality:**
  - Uses SerpAPI to search Google Scholar with a complex query related to machine learning, obstructive sleep apnea, and screening questionnaires.
  - Extracts key metadata (title, abstract snippet, and link) from search results.
  - Filters articles based on predefined inclusion and exclusion criteria using spaCy for NLP and fuzzy matching for title deduplication.
  - Saves the final list of relevant studies to `google_scholar_osa_ml_tracking.csv`.

- **Key Methods:**
  - `search_google_scholar(query, max_pages)`: Searches multiple pages on Google Scholar.
  - `is_relevant(abstract, inclusion_criteria, exclusion_criteria)`: Checks if an abstract meets the criteria.
  - `remove_duplicates(df)`: Removes duplicate entries based on title similarity.

- **Usage:**

  Run the script directly:

  ```bash
  python MetaResearchStep1_FindPapers.py
  ```

### MetaResearchStep2_GetFullAbstracts.py

- **Functionality:**
  - Uses Selenium to access web pages for each paper link extracted by the first script.
  - Uses a Hugging Face text-generation model (such as Llama-2-7B-chat-hf) to extract a full abstract from the page text.
  - Adds a new column (`complete_sbtract`) to the DataFrame and saves the results to `output_with_abstracts.csv`.

- **Key Methods:**
  - `hf_extract_abstract(page_text)`: Sends a prompt to the Hugging Face model to extract a concise abstract from the page text.
  - Selenium setup for headless browsing and automated page retrieval.

- **Usage:**

  Make sure to update the paths for:
  - Chrome binary location.
  - CSV file path in the script.
  
  Then, run the script:

  ```bash
  python MetaResearchStep2_GetFullAbstracts.py
  ```

### ScrapperPubMed.py

- **Functionality:**
  - Constructs a search query for PubMed and scrapes the search results for paper titles and links.
  - Iterates through the results and extracts abstracts from individual paper pages using BeautifulSoup.
  - Saves the final data to `pubmed_results.csv`.

- **Key Methods:**
  - `fetch_search_results(query, max_pages)`: Retrieves search results over a specified number of pages.
  - `fetch_abstract(paper_url)`: Extracts the abstract from a given PubMed page.
  - `save_to_csv(papers, filename)`: Writes the collected data to a CSV file.
  - `main()`: Orchestrates the entire scraping workflow including user input for the number of pages to scrape.

- **Usage:**

  Run the script and follow the on-screen prompt to specify the number of pages:

  ```bash
  python ScrapperPubMed.py
  ```

## EMBASE
If we use the embase database for the search and use the csv file that it outputs, we will need to use the script `structure_embase.pyt` to structure the output into columns wise attributes for each paper (row). 

Furthermore, we will use the script `filter_emabe.pyt` to exclude papers that are of no interest. 

## Usage

1. **Step 1: Find Papers**
   - Run `MetaResearchStep1_FindPapers.py` to search for relevant papers on Google Scholar and create a CSV file (`google_scholar_osa_ml_tracking.csv`).

2. **Step 2: Extract Full Abstracts**
   - Run `MetaResearchStep2_GetFullAbstracts.py` to process the CSV file from Step 1. This will extract detailed abstracts using Selenium and a Hugging Face model, and save them to `output_with_abstracts.csv`.

3. **Alternative Source: PubMed Scraping**
   - Run `ScrapperPubMed.py` to fetch papers and abstracts directly from PubMed. The script will prompt for the number of pages to scrape and output the results to `pubmed_results.csv`.

4. **Use www.embase.com**
  - Here we can manually place the same query and it genrates for us the .csv as we did for PubMed and Google Scholar 
  - We can use the LLMS to read through the abstracts and filter for us
## Notes

- **API Keys & Tokens:** Ensure all API keys (SerpAPI, Hugging Face) and tokens are correctly set before running the scripts.
- **Rate Limiting:** Some functions include delays (using `time.sleep`) to avoid rate limiting. Adjust these delays if necessary.
- **Customization:** Modify the query strings and filtering criteria in the scripts to tailor the search to your specific research needs.
- **Error Handling:** Basic error handling is implemented. Review console messages for any issues during execution.
